{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP Model on New Data Rev 1.0\n",
    "\n",
    "This program attempts to load a nlp classifier model and use it on a new/different data set other than that which created the model.\n",
    "\n",
    "The export/import of the model and the vocabulary file are done through the pickle library\n",
    "The extension of the model file and the vocabulary file is .sav\n",
    "\n",
    "The first step is to load the data to be analyzed by the model. (.csv file)\n",
    "\n",
    "The second step is to load the actual model and the vocabulary file. (.sav files) Both files are necessary.\n",
    "\n",
    "Each are created when the model is created and saved through the use of pickle, which in essence saves the file as a binary file. Both of these files must be saved when the model is created.\n",
    "\n",
    "https://medium.com/@maziarizadi/pickle-your-model-in-python-2bbe7dba2bbb\n",
    "https://www.kdnuggets.com/2019/11/create-vocabulary-nlp-tasks-python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions =  [1. 1. 1. ... 1. 0. 0.]\n",
      "bullish =  663\n",
      "neutral =  580\n",
      "bearish =  0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk.classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "###########################################################\n",
    "#### METHODS                                           ####\n",
    "###########################################################\n",
    "\n",
    "# displays a list of file with on a csv suffix       \n",
    "def list_dir_files(relevant_path):\n",
    "    # https://clay-atlas.com/us/blog/2019/10/27/python-english-tutorial-solved-unicodeescape-error-escape-syntaxerror/?doing_wp_cron=1618286551.1528689861297607421875\n",
    "    #need to change \\ to /\n",
    "\n",
    "    import os\n",
    "    \n",
    "    included_extensions = ['csv']\n",
    "    file_names = [fn for fn in os.listdir(relevant_path) # uses os.listdir to display only .csv files\n",
    "              if any(fn.endswith(ext) for ext in included_extensions)]\n",
    "\n",
    "    print('Path: ', relevant_path)\n",
    "\n",
    "    for f in file_names:\n",
    "        print(f)\n",
    "        \n",
    "# initializes the dataframe \"df\" and imports the csv into df; \n",
    "# the argument is the name/address of the file.\n",
    "# https://stackoverflow.com/questions/33440805/pandas-dataframe-read-csv-on-bad-data\n",
    "def getData(name):\n",
    "    df1 = pd.DataFrame() # defines df1 as a dataframe\n",
    "    df1 = pd.read_csv(name, header = 0)\n",
    "    return df1\n",
    "\n",
    "\n",
    "############################################################\n",
    "####   MAIN                                             ####\n",
    "############################################################\n",
    "\n",
    "#Loads a specific model and vocabulary file:\n",
    "#ModelFileToLoad = 'C:/Users/pstri/OneDrive/Documents/Personal/Kokoro/NLTK/Code Project/NLP Models/preprocessed r_a tech stockTwit 03112021Model.sav'\n",
    "#VocabFileToLoad = 'C:/Users/pstri/OneDrive/Documents/Personal/Kokoro/NLTK/Code Project/NLP Models/preprocessed r_a tech stockTwit 03112021Vocab.sav'\n",
    "\n",
    "ModelFileToLoad = 'preprocessed r_a tech stockTwit 03112021Model.sav'\n",
    "VocabFileToLoad = 'preprocessed r_a tech stockTwit 03112021Vocab.sav'\n",
    "\n",
    "loaded_model = pickle.load(open(ModelFileToLoad, 'rb'))\n",
    "loaded_vocabulary = pickle.load(open(VocabFileToLoad, 'rb'))\n",
    "\n",
    "'''#Calls getData to import the csv into the dataframe. This file is has the data that we want to perform the sentiment analysis on.\n",
    "relevant_path = 'C:/Users/pstri/OneDrive/Documents/Personal/Kokoro/NLTK/Code Project/Scraped Files'\n",
    "\n",
    "print('Here are the list of csv files to choose from: ')\n",
    "list_dir_files(relevant_path) # method to list files in the directory where this file is found.\n",
    "'''\n",
    "time.sleep(1)\n",
    "\n",
    "# name = input('\\nWhat is the name of the csv file you want to import? \\n')\n",
    "\n",
    "name = 'preprocessed r_d r_a r_e_o tech stockTwit 03112021.csv'\n",
    "dfAPI = getData(name)\n",
    "\n",
    "fileName = name\n",
    "\n",
    "####\n",
    "# Need to convert sanitized data into vectors\n",
    "####\n",
    "\n",
    "# converts the df columns of body and the label (compound or sentiment_number) into one list for each column\n",
    "#this is needed to be able to create the \n",
    "tech_twits_text = dfAPI['body'].tolist() #'body' is the column that contains the training data\n",
    "\n",
    "''' There is no need for the following steps because they are captured in the 'loaded_vocabulary' file.\n",
    "The vocabulary was created when the model was created.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "counter = CountVectorizer() #creates a CountVectorizer named counter\n",
    "counter.fit(tech_twits_text) #teaches the counter the vocabulary\n",
    "\n",
    "The loaded_vocabulary' file is set equal to counter and then counter is used to transform the data into a Count Vector.\n",
    "This is just like when the model was created. The model then is applied to the Count Vector to determine sentiment.\n",
    "\n",
    "'''\n",
    "counter = loaded_vocabulary\n",
    "\n",
    "tech_twits_text_counts = counter.transform(tech_twits_text) #transforms train_data to a Count Vector which will be the input to the model for sentiment analysis\n",
    "\n",
    "predictions = loaded_model.predict(tech_twits_text_counts) #predicts the value (label/sentiment) from the tech_twits_text_counts \n",
    "\n",
    "print('predictions = ', predictions)\n",
    "\n",
    "\n",
    "len(predictions) #gives the length of the array\n",
    "\n",
    "type(predictions) #gives the data type of predictions\n",
    "\n",
    "copy_predictions = predictions #makes a copy of predictions\n",
    "\n",
    "df = pd.DataFrame(data = copy_predictions, columns = ['Sentiment']) #creates a dataframe from 'copy_predictions'\n",
    "\n",
    "#display(df)\n",
    "\n",
    "#itemizes the predicted sentiment values\n",
    "bullish = 0\n",
    "neutral = 0\n",
    "bearish = 0\n",
    "\n",
    "i = 0\n",
    "\n",
    "while i < len(df):\n",
    "    if df.iloc[i,0] == 1:\n",
    "        bullish += 1\n",
    "    elif df.iloc[i,0] == 0:\n",
    "        neutral += 1\n",
    "    elif df.iloc[i,0] == -1:\n",
    "        bearish = 0\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "print('bullish = ', bullish)\n",
    "print('neutral = ', neutral)\n",
    "print('bearish = ', bearish)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
