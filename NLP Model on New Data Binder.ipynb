{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP Model on New Data Rev 1.0\n",
    "\n",
    "This program attempts to load a nlp classifier model and use it on a new/different data set other than that which created the model.\n",
    "\n",
    "The export/import of the model and the vocabulary file are done through the pickle library\n",
    "The extension of the model file and the vocabulary file is .sav\n",
    "\n",
    "The first step is to load the data to be analyzed by the model. (.csv file)\n",
    "\n",
    "The second step is to load the actual model and the vocabulary file. (.sav files) Both files are necessary.\n",
    "\n",
    "Each are created when the model is created and saved through the use of pickle, which in essence saves the file as a binary file. Both of these files must be saved when the model is created.\n",
    "\n",
    "https://medium.com/@maziarizadi/pickle-your-model-in-python-2bbe7dba2bbb\n",
    "https://www.kdnuggets.com/2019/11/create-vocabulary-nlp-tasks-python.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions =  [1. 1. 1. ... 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk.classify\n",
    "from nltk import NaiveBayesClassifier\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "###########################################################\n",
    "#### METHODS                                           ####\n",
    "###########################################################\n",
    "\n",
    "# displays a list of file with on a csv suffix       \n",
    "def list_dir_files(relevant_path):\n",
    "    # https://clay-atlas.com/us/blog/2019/10/27/python-english-tutorial-solved-unicodeescape-error-escape-syntaxerror/?doing_wp_cron=1618286551.1528689861297607421875\n",
    "    #need to change \\ to /\n",
    "\n",
    "    import os\n",
    "    \n",
    "    included_extensions = ['csv']\n",
    "    file_names = [fn for fn in os.listdir(relevant_path) # uses os.listdir to display only .csv files\n",
    "              if any(fn.endswith(ext) for ext in included_extensions)]\n",
    "\n",
    "    print('Path: ', relevant_path)\n",
    "\n",
    "    for f in file_names:\n",
    "        print(f)\n",
    "        \n",
    "# initializes the dataframe \"df\" and imports the csv into df; \n",
    "# the argument is the name/address of the file.\n",
    "# https://stackoverflow.com/questions/33440805/pandas-dataframe-read-csv-on-bad-data\n",
    "def getData(name):\n",
    "    df1 = pd.DataFrame() # defines df1 as a dataframe\n",
    "    df1 = pd.read_csv(name, header = 0)\n",
    "    return df1\n",
    "\n",
    "\n",
    "############################################################\n",
    "####   MAIN                                             ####\n",
    "############################################################\n",
    "\n",
    "#Loads a specific model and vocabulary file:\n",
    "#ModelFileToLoad = 'C:/Users/pstri/OneDrive/Documents/Personal/Kokoro/NLTK/Code Project/NLP Models/preprocessed r_a tech stockTwit 03112021Model.sav'\n",
    "#VocabFileToLoad = 'C:/Users/pstri/OneDrive/Documents/Personal/Kokoro/NLTK/Code Project/NLP Models/preprocessed r_a tech stockTwit 03112021Vocab.sav'\n",
    "\n",
    "ModelFileToLoad = 'preprocessed r_a tech stockTwit 03112021Model.sav'\n",
    "VocabFileToLoad = 'preprocessed r_a tech stockTwit 03112021Vocab.sav'\n",
    "\n",
    "loaded_model = pickle.load(open(ModelFileToLoad, 'rb'))\n",
    "loaded_vocabulary = pickle.load(open(VocabFileToLoad, 'rb'))\n",
    "\n",
    "'''#Calls getData to import the csv into the dataframe. This file is has the data that we want to perform the sentiment analysis on.\n",
    "relevant_path = 'C:/Users/pstri/OneDrive/Documents/Personal/Kokoro/NLTK/Code Project/Scraped Files'\n",
    "\n",
    "print('Here are the list of csv files to choose from: ')\n",
    "list_dir_files(relevant_path) # method to list files in the directory where this file is found.\n",
    "'''\n",
    "time.sleep(1)\n",
    "\n",
    "# name = input('\\nWhat is the name of the csv file you want to import? \\n')\n",
    "\n",
    "name = 'preprocessed r_d r_a r_e_o tech stockTwit 03112021.csv'\n",
    "dfAPI = getData(name)\n",
    "\n",
    "fileName = name\n",
    "\n",
    "####\n",
    "# Need to convert sanitized data into vectors\n",
    "####\n",
    "\n",
    "# converts the df columns of body and the label (compound or sentiment_number) into one list for each column\n",
    "#this is needed to be able to create the \n",
    "tech_twits_text = dfAPI['body'].tolist() #'body' is the column that contains the training data\n",
    "\n",
    "''' There is no need for the following steps because they are captured in the 'loaded_vocabulary' file.\n",
    "The vocabulary was created when the model was created.\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "counter = CountVectorizer() #creates a CountVectorizer named counter\n",
    "counter.fit(tech_twits_text) #teaches the counter the vocabulary\n",
    "\n",
    "The loaded_vocabulary' file is set equal to counter and then counter is used to transform the data into a Count Vector.\n",
    "This is just like when the model was created. The model then is applied to the Count Vector to determine sentiment.\n",
    "\n",
    "'''\n",
    "counter = loaded_vocabulary\n",
    "\n",
    "tech_twits_text_counts = counter.transform(tech_twits_text) #transforms train_data to a Count Vector which will be the input to the model for sentiment analysis\n",
    "\n",
    "predictions = loaded_model.predict(tech_twits_text_counts) #predicts the value (label/sentiment) from the tech_twits_text_counts \n",
    "\n",
    "print('predictions = ', predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1286</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1287</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1288</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1289</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1290</th>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1291 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sentiment\n",
       "0           1.0\n",
       "1           1.0\n",
       "2           1.0\n",
       "3           1.0\n",
       "4           0.0\n",
       "...         ...\n",
       "1286        0.0\n",
       "1287        1.0\n",
       "1288        1.0\n",
       "1289        0.0\n",
       "1290        0.0\n",
       "\n",
       "[1291 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bullish =  663\n",
      "neutral =  580\n",
      "bearish =  0\n"
     ]
    }
   ],
   "source": [
    "len(predictions) #gives the length of the array\n",
    "\n",
    "type(predictions) #gives the data type of predictions\n",
    "\n",
    "copy_predictions = predictions #makes a copy of predictions\n",
    "\n",
    "df = pd.DataFrame(data = copy_predictions, columns = ['Sentiment']) #creates a dataframe from 'copy_predictions'\n",
    "\n",
    "display(df)\n",
    "\n",
    "#itemizes the predicted sentiment values\n",
    "bullish = 0\n",
    "neutral = 0\n",
    "bearish = 0\n",
    "\n",
    "i = 0\n",
    "\n",
    "while i < len(df):\n",
    "    if df.iloc[i,0] == 1:\n",
    "        bullish += 1\n",
    "    elif df.iloc[i,0] == 0:\n",
    "        neutral += 1\n",
    "    elif df.iloc[i,0] == -1:\n",
    "        bearish = 0\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "print('bullish = ', bullish)\n",
    "print('neutral = ', neutral)\n",
    "print('bearish = ', bearish)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''def getData(csv_name):\n",
    "    df1 = pd.DataFrame() # defines df1 as a dataframe\n",
    "    df1 = pd.read_csv(csv_name, header = 0, encoding = 'utf-8') # reads the csv file into df1 (dataframe)\n",
    "    return df1\n",
    "\n",
    "def df_to_model_accuracy(df, filename):\n",
    "    # 160 converts the df columns of body and the label (compound or sentiment_number) into one list for each column\n",
    "    #this is needed to be able to create the \n",
    "    tech_twits_text = df['body'].tolist() # 'body' is the column that contains the training data\n",
    "    compound_bin_labels = df['compound_bin'].tolist() # 'compound_bin are the numbers that represent the nltk Vader compound sentiment numbers as 1,0,-1'\n",
    "    sentiment_number_labels = df['sentiment_number'].tolist() #'sentiment_number' column is the integer tranlsation from the stocktwits 'bullish', 'None', 'bearish' sentiment rankings\n",
    "    #labels = df['modified_rating'].tolist()\n",
    "    #labels = df['Rating'].tolist()\n",
    "    #labels = df['raw_compound'].tolist()\n",
    "\n",
    "    # 170 divides the whole data set into a 80/20 split into a training set and a test set\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    # train_labels are from Vader's compound bin\n",
    "    train_data, test_data, train_labels, test_labels = train_test_split(tech_twits_text, compound_bin_labels, test_size = 0.2, random_state = 1)\n",
    "    \n",
    "    # sntrain_labels are from the stocktwits sentiment self reported sentiment values.\n",
    "    sntrain_data, sntest_data, sntrain_labels, sntest_labels = train_test_split(tech_twits_text, sentiment_number_labels, test_size = 0.2, random_state = 1)\n",
    "\n",
    "    # 190 Converts the text into vectors based on word usage\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    counter = CountVectorizer() #creates a CountVectorizer named counter\n",
    "    counter.fit(train_data) #teaches the counter our vocabulary\n",
    "    \n",
    "    ## for VADER\n",
    "    train_counts = counter.transform(train_data) #transforms train_data to a Count Vector\n",
    "    test_counts = counter.transform(test_data) #transforms test_data to a Count Vector\n",
    "\n",
    "    ## for Stocktwits sentiment\n",
    "    sntrain_counts = counter.transform(sntrain_data) #transforms train_data to a Count Vector\n",
    "    sntest_counts = counter.transform(sntest_data) #transforms test_data to a Count Vector    \n",
    "    \n",
    "    # 200 creates the model based on the vectors and applies (predictions) the model to the test data set\n",
    "    #https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html\n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    classifierVader = MultinomialNB()\n",
    "    classifierStocktwits = MultinomialNB()\n",
    "    \n",
    "    ## for VADER\n",
    "    classifierVader.fit(train_counts, train_labels)\n",
    "    predictions = classifierVader.predict(test_counts) # predicts the value (label/sentiment) from the test_counts is the test_data \n",
    "\n",
    "    ## for Stocktwits sentiment\n",
    "    classifierStocktwits.fit(sntrain_counts, sntrain_labels)\n",
    "    snpredictions = classifierStocktwits.predict(test_counts) # predicts the value (label/sentiment) from the test_counts is the test_data \n",
    "    \n",
    "    print('This compares the multinomial naive bayes sentiment model [MNBSM] using different inputs and outputs')\n",
    "    print('The inputs were either produced by Vader or taken directly from Stocktwits.')\n",
    "    print('The labels were either produced by Vader or taken directily from Stocktwits.')\n",
    "    print('All comparisons are with the MNBSM but with different inputs and different labels.')\n",
    "    \n",
    "    # 230 compares the accuracy of the predicted values and the real test values (labels)\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    print('accuracy score of Stocktwits model to Stocktwits: \\n', accuracy_score(sntest_labels, snpredictions), '\\n')  \n",
    "    print('accuracy score of Vader to Vader: \\n', accuracy_score(test_labels, predictions), '\\n')\n",
    "    print('accuracy score of Vader to Stocktwits: \\n', accuracy_score(sntest_labels, predictions), '\\n')\n",
    "\n",
    "    # 240 compares the precision of the predicted values to the real test values (labels)\n",
    "    #https://nlpforhackers.io/classification-performance-metrics/\n",
    "    #https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html\n",
    "    from sklearn.metrics import precision_score\n",
    "    print('precision score of Stocktwits model to Stocktwits: \\n', precision_score(sntest_labels, snpredictions, average = 'micro'), '\\n')  \n",
    "    print('precision score of Vader to Vader: \\n', precision_score(test_labels, predictions, average = 'micro'), '\\n')\n",
    "    print('precision score of Vader to Stocktwits: \\n', precision_score(sntest_labels, predictions, average = 'micro'), '\\n')\n",
    "    \n",
    "    # 250 Confusion Matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    print('confusion matrix of Stocktwits model to Stocktwits: \\n', confusion_matrix(sntest_labels, snpredictions), '\\n')\n",
    "    print('confusion matrix of Vader to Vader: \\n', confusion_matrix(test_labels, predictions), '\\n')\n",
    "    print('confusion matrix of Vader to Stocktwits: \\n', confusion_matrix(sntest_labels, predictions), '\\n')\n",
    "    \n",
    "    #260 Classification Report\n",
    "    from sklearn.metrics import classification_report\n",
    "    print('classification report of Stocktwits to Stocktwits: \\n', classification_report(sntest_labels, snpredictions))\n",
    "    print('classification report of Vader to Vader: \\n', classification_report(test_labels, predictions))\n",
    "    print('classification report of Vader to Stocktwits: \\n', classification_report(sntest_labels, predictions))\n",
    "\n",
    "    \n",
    "    ########\n",
    "    # EXPORT MODEL\n",
    "    ########\n",
    "\n",
    "    csv_filename = filename\n",
    "\n",
    "    model_filename = csv_filename.replace('.csv', '.sav')\n",
    "\n",
    "    print('The filename of the model is: ', model_filename)\n",
    "\n",
    "    pickle.dump(classifierStocktwits, open(model_filename, 'wb'))\n",
    "\n",
    "    \n",
    "    # some time later...\n",
    " \n",
    "    # load the model from disk\n",
    "    loaded_model = pickle.load(open(filename, 'rb'))\n",
    "    result = loaded_model.score(sntrain_counts, sntrain_labels)\n",
    "    print(result)\n",
    "    \n",
    "    \n",
    "    \n",
    "# prints out the names of the files in the directory\n",
    "def list_dir_files(relevant_path):\n",
    "    #https://clay-atlas.com/us/blog/2019/10/27/python-english-tutorial-solved-unicodeescape-error-escape-syntaxerror/?doing_wp_cron=1618286551.1528689861297607421875\n",
    "    #need to change \\ to /\n",
    "\n",
    "    import os\n",
    "\n",
    "    included_extensions = ['csv']\n",
    "    file_names = [fn for fn in os.listdir(relevant_path)\n",
    "              if any(fn.endswith(ext) for ext in included_extensions)]\n",
    "\n",
    "    for f in file_names:\n",
    "        print(f)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/61967427/i-am-getting-an-vocabulary-not-fitted-or-provided\n",
    "https://www.programcreek.com/python/example/103577/sklearn.exceptions.NotFittedError\n",
    "\n",
    "https://stackoverflow.com/questions/60472925/python-scikit-svm-vocabulary-not-fitted-or-provided\n",
    "you need to use the vectorizer on which you have trained the model. in your code you are creating object for tfidfvectorizer and than using it with transform, which will throw the error.. so you need to save vectorizer when training the model and than using the same vectorizer for prediction – qaiser Mar 2 '20 at 7:01 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
